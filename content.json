{"pages":[],"posts":[{"title":"计算时间间隔","text":"使用C语言实现。 计算两个日期到 0 年 3 月 1 日的间隔，间隔之差即为结果。 m1 = (month_start + 9) % 12：到3月的间隔月数 y1 = year_start - m1/10：当月份为1或2月时年数减1 365*y1：不计闰年多出那一天时的天数 y1/4 - y1/100 + y1/400：闰年多出的天数 (m2*306 + 5)/10：当前月份到3月1日的天数 306=365-31-28 5：全年中不是31天月份的个数 day_start - 1：到1号的天数 1234567891011121314151617181920212223242526int day_diff(int year_start, int month_start, int day_start, int year_end, int month_end, int day_end){ int y2, m2, d2; int y1, m1, d1; m1 = (month_start + 9) % 12; y1 = year_start - m1 / 10; d1 = 365 * y1 + y1 / 4 - y1 / 100 + y1 / 400 + (m1 * 306 + 5) / 10 + (day_start - 1); m2 = (month_end + 9) % 12; y2 = year_end - m2 / 10; d2 = 365 * y2 + y2 / 4 - y2 / 100 + y2 / 400 + (m2 * 306 + 5) / 10 + (day_end - 1); return (d2 - d1);}int main(){ printf(&quot;%d\\n&quot;, day_diff(2015, 1, 1, 2015, 1, 8)); printf(&quot;%d\\n&quot;, day_diff(2015, 1, 29, 2015, 2, 9)); printf(&quot;%d\\n&quot;, day_diff(2019, 1, 1, 2019, 2, 1)); printf(&quot;%d\\n&quot;, day_diff(2019, 1, 1, 2020, 1, 1)); return 0;} 另：如需计算分钟间隔 $$time = days * 60 * 24 + (H2 - H1) * 60 + (m2 - m1)$$","link":"/2019/12/26/%E8%AE%A1%E7%AE%97%E6%97%A5%E6%9C%9F%E9%97%B4%E9%9A%94/"},{"title":"TensorFlow基础","text":"TensorFlow基础代码。 使用graph表示计算任务12345678910import tensorflow as tfm1 = tf.constant([[3, 3]])# Create an opm2 = tf.constant([[2], [3]])# Create an opproduct = tf.matmul(m1, m2)# Create an op, Get a tensorwith tf.Session() as sess: result = sess.run(product) print(result) 变量的使用123456789101112131415161718192021222324252627282930import tensorflow as tfx = tf.Variable([1, 2])a = tf.constant([3, 3])sub = tf.subtract(x,a)add = tf.add(x,sub)init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) print(sess.run(sub)) print(sess.run(add))# ---------Another example# Create a variable and initializer it to be 0state = tf.Variable(0,name='counter')new_value = tf.add(state,1)# Give the value of 'new_value' to 'state'update = tf.assign(state, new_value)with tf.Session() as sess: sess.run(init) print(sess.run(state)) for _ in range(5): sess.run(update) print(sess.run(state)) Fetch and Feed123456789101112131415161718192021222324import tensorflow as tf# ----------Fetchinput1 = tf.constant(3.0)input2 = tf.constant(2.0)input3 = tf.constant(5.0)add = tf.add(input2, input3)mul = tf.multiply(input1, add)with tf.Session() as sess: # Fetch: run many ops at the same time result = sess.run([mul, add]) print(result)#-------------Feedinput1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.multiply(input1, input2)with tf.Session() as sess: print(sess.run(output,feed_dict={input1:[i for i in range(1,5)],input2:[j for j in range(2,6)]})) 一个简单的使用案例123456789101112131415161718192021222324import tensorflow as tfimport numpy as np# Use numpy to product 100 random numbersx_data = np.random.rand(100)y_data = x_data*0.1 + 0.2b = tf.Variable(0.)k = tf.Variable(0.)y = k*x_data + bloss = tf.reduce_mean(tf.square(y_data-y))# Define an optimizeroptimizer = tf.train.GradientDescentOptimizer(0.2)train = optimizer.minimize(loss)init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for step in range(20): sess.run(train) if step%2 == 0: print(step, sess.run([k, b])) 手写数字数据集和softmax MNIST数据集 60000行的训练数据集和10000行的测试数据集 每张图片28*28=784个像素 训练集张量形状为[60000, 784]，即[索引图片，索引像素点] one-hot vectors，mnist.train.labels形状为[60000, 10] Softmax函数 $$\\operatorname{softmax}(x){i}=\\frac{\\exp \\left(x{i}\\right)}{\\sum_{j} \\exp \\left(x_{j}\\right)}$$ 一个非线性回归的例子 12345678910111213141516171819202122232425262728293031323334import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltx_data = np.linspace(-0.5, 0.5, 200)[:, np.newaxis]noise = np.random.normal(0, 0.02, x_data.shape)y_data = np.square(x_data) + noisex = tf.placeholder(tf.float32, [None, 1])y = tf.placeholder(tf.float32, [None, 1])Weight_L1 = tf.Variable(tf.random_normal([1, 10]))biases_L1 = tf.Variable(tf.zeros([1, 10]))Wx_plus_b_L1 = tf.matmul(x, Weight_L1) + biases_L1L1 = tf.nn.tanh(Wx_plus_b_L1)Weights_L2 = tf.Variable(tf.random_normal([10, 1]))biases_L2 = tf.Variable(tf.zeros([1, 1]))Wx_plus_b_L2 = tf.matmul(L1, Weights_L2) + biases_L2prediction = tf.nn.tanh(Wx_plus_b_L2)loss = tf.reduce_mean(tf.square(y - prediction))train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for _ in range(2000): sess.run(train_step, feed_dict={x: x_data, y: y_data}) prediction_value = sess.run(prediction, feed_dict={x: x_data}) plt.figure() plt.scatter(x_data, y_data) plt.plot(x_data, prediction_value, 'r-', lw=5) plt.show() 训练时不断更新Weight_L1，biases_L1，Weights_L2，biases_L2。 手写数字识别（简单版本）12345678910111213141516171819202122232425262728293031323334import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)batch_size = 100# Calculate the number of batchn_batch = mnist.train.num_examples // batch_sizex = tf.placeholder(tf.float32,[None,784])y = tf.placeholder(tf.float32,[None,10])# Create a simple neural network without hidden layerW = tf.Variable(tf.zeros([784,10]))b = tf.Variable(tf.zeros([10]))prediction = tf.nn.softmax(tf.matmul(x,W) + b)loss = tf.reduce_mean(tf.square(y-prediction))train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)init = tf.global_variables_initializer()# Get a list of &quot;bool&quot; typecorrect_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))with tf.Session() as sess: sess.run(init) for epoch in range(21): for batch in range(n_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step,feed_dict={x:batch_xs, y:batch_ys}) acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels}) print(&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy &quot; + str(acc)) 输出：123456789101112131415161718192021Iter 0, Testing Accuracy 0.8338Iter 1, Testing Accuracy 0.8715Iter 2, Testing Accuracy 0.8825Iter 3, Testing Accuracy 0.8881Iter 4, Testing Accuracy 0.894Iter 5, Testing Accuracy 0.897Iter 6, Testing Accuracy 0.9003Iter 7, Testing Accuracy 0.9012Iter 8, Testing Accuracy 0.9043Iter 9, Testing Accuracy 0.9056Iter 10, Testing Accuracy 0.9058Iter 11, Testing Accuracy 0.9075Iter 12, Testing Accuracy 0.9078Iter 13, Testing Accuracy 0.909Iter 14, Testing Accuracy 0.91Iter 15, Testing Accuracy 0.911Iter 16, Testing Accuracy 0.9115Iter 17, Testing Accuracy 0.9123Iter 18, Testing Accuracy 0.9133Iter 19, Testing Accuracy 0.9131Iter 20, Testing Accuracy 0.9134 注释： tf.argmax()函数中有个axis参数（轴），该参数能指定按照哪个维度计算。0：按列计算，1：行计算。 tf.cast(x, dtype, name=None)。x：待转换的数据（张量），dtype：目标数据类型。 reduce_mean(input_tensor,axis=None,keep_dims=False,name=None)函数用于计算张量 tensor 沿着指定的数轴（ tensor 的某一维度）上的的平均值，主要用作降维或者计算 tensor（图像）的平均值。 tf.reduce_sum ：计算 tensor 指定轴方向上的所有元素的累加和; tf.reduce_max : 计算 tensor 指定轴方向上的各个元素的最大值; tf.reduce_all : 计算 tensor 指定轴方向上的各个元素的逻辑和（and运算）; tf.reduce_any: 计算 tensor 指定轴方向上的各个元素的逻辑或（or运算）; 改进方向（不改变方法）： 改变batch_size大小 添加隐藏层，隐藏层的激活函数、神经元个数 权值和偏置值的初始值 代价函数尝试使用交叉熵 优化方式，学习率 增加epoch个数 拟合防止过拟合： 增加数据集 正则化方法 Dropout Dropout 的使用：只让一部分（keep_prob）神经元工作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;MNIST_data&quot;,one_hot=True)batch_size = 100# Calculate the number of batchn_batch = mnist.train.num_examples // batch_sizex = tf.placeholder(tf.float32,[None,784])y = tf.placeholder(tf.float32,[None,10])keep_prob = tf.placeholder(tf.float32)# Create a simple nerual network without hiddenW1 = tf.Variable(tf.truncated_normal([784,2000],stddev=0.1))b1 = tf.Variable(tf.zeros([2000])+0.1)L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)L1_dropout = tf.nn.dropout(L1,keep_prob)W2 = tf.Variable(tf.truncated_normal([2000,2000],stddev=0.1))b2 = tf.Variable(tf.zeros([2000])+0.1)L2 = tf.nn.tanh(tf.matmul(L1_dropout,W2)+b2)L2_dropout = tf.nn.dropout(L2,keep_prob)W3= tf.Variable(tf.truncated_normal([2000,1000],stddev=0.1))b3 = tf.Variable(tf.zeros([1000])+0.1)L3 = tf.nn.tanh(tf.matmul(L2_dropout,W3)+b3)L3_dropout = tf.nn.dropout(L3,keep_prob)W4= tf.Variable(tf.truncated_normal([1000,10],stddev=0.1))b4 = tf.Variable(tf.zeros([10])+0.1)prediction = tf.nn.softmax(tf.matmul(L3_dropout,W4) + b4)# loss = tf.reduce_mean(tf.square(y-prediction))loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=tf.matmul(L3_dropout,W4) + b4))train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)init = tf.global_variables_initializer()# Get a list of &quot;bool&quot; typecorrect_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))num_epoch = 30with tf.Session() as sess: sess.run(init) for epoch in range(num_epoch): for batch in range(n_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step,feed_dict={x:batch_xs, y:batch_ys,keep_prob:1.0}) test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0}) train_acc = sess.run(accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels, keep_prob: 1.0}) print(&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy &quot; + str(test_acc)) print(&quot;Iter &quot; + str(epoch) + &quot;, Train Accuracy &quot; + str(train_acc)) CNN相关概念 局部感受野：CNN通过局部感受野和权值共享减少了神经网络需要训练的参数个数 卷积操作 池化：max-pooling 和 mean-pooling Padding SAME PADDING 卷积：给平面外部补0，卷积窗口采样后得到一个跟原来平面大小相同的平面。 池化：可能会给外部补0。 VALID PADDING 卷积：不会给外部补0。卷积窗口采样后得到一个比原来平面小的平面。 池化：不会给外部补0。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets('MNIST_data', one_hot=True)batch_size = 100n_batch = mnist.train.num_examples // batch_sizedef weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)def conv2d(x, W): # x input: A 4-D tensor [batch, in_height, in_width, in_channels] # W filter: A 4-D tensor [filter_height, filter_width, in_channels, out_channels] # strides: A list of `ints`. strides[0] = strides[3] = 1. # strides[1] and strides[2] stand for strides for x &amp; y direction return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): # ksize [1, x, y, 1] return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')x = tf.placeholder(tf.float32, [None, 784])y = tf.placeholder(tf.float32, [None, 10])x_image = tf.reshape(x, [-1, 28, 28, 1])# The first layerW_conv1 = weight_variable([5, 5, 1, 32])b_conv1 = bias_variable([32])h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1)# The second layerW_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)# The third layerW_fc1 = weight_variable([7*7*64, 1024])b_fc1 = weight_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)keep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)W_fc2 = weight_variable([1024, 10])b_fc2 = weight_variable([10])prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(21): for batch in range(n_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) sess.run(train_step, feed_dict = {x: batch_xs, y: batch_ys, keep_prob: 0.7}) acc = sess.run(accuracy,feed_dict = {x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}) print(&quot;Iter &quot; + str(epoch) + &quot;, Testing Accuracy = &quot; + str(acc)) 保存模型123456saver = tf.train.Saver()with tf.Session() as sess: sess.run(init) # train the model saver.save(sess, 'net/my_net.ckpt') 读取模型123with tf.Session() as sess: sess.run(init) saver.restore(sess, 'net/my_net.ckpt')","link":"/2020/01/31/TensorFlow%E5%9F%BA%E7%A1%80/"},{"title":"Collections源码解析","text":"collections 是 Python 内建的集合模块，提供了一些有关集合的函数和类。 一、背景知识1、内置属性和方法 __all__用于限制作为模块导入时，允许导入的变量、属性、方法和类，包括私有的。在如下的示例代码中，只有类A和函数_func能够被导入，而类B不能。 1234__all__=('A', '_func')class A: passclass B: passdef _func(): pass __dict__属性存放了类或对象的一些数据。在类的__dict__中，存放了类方法、静态方法、普通方法和全局变量，数据类型为mappingproxy；而在对象的__dict__中，则存放了一些对象自身的数据self.xxx，数据类型为dict。 __slots__限制了能够为类添加的属性列表。如对于如下类A及其实例a，能够为其添加属性a.age和a.name，但不能添加a.school。 123class A(): __slots__ = ['age', 'name']a = A() __sizeof__函数返回系统分配对象空间的大小。 __doc__返回函数的注释内容，__name__返回函数的名称，__file__返回函数的名称，__qualname__返回函数的限定名。 对于任意对象obj，obj.__class__(obj)返回自身对象的复制。 exec(object[, globals[, locals]])能够运行传入的 Python 语句，并从参数中获取全局变量和局部变量。 2、魔术方法魔术方法是指 Python 中形如__func__的方法，这些方法会在特定情况下由其他函数自动调用。 __new__(cls[, ...])方法在创建类的实例时，在__init__方法之前调用。 __init__(self[, ...])是构造方法，__del__(self)是析构方法。 有一些方法__func__(self)，会在调用func(obj)时被调用，包括：__len__、__repr__、__str__、__bytes__、__hash__、__bool__、__format__ 、__reversed__。 重载比较操作符的方法：__lt__、__le__、__eq__、__ne__、__gt__、__ge__分别对应&lt;、&lt;=、==、!=、&gt;、&gt;=。 重载算术运算符的方法：__add__、__sub__、__mul__、__truediv__、__floordiv__、__mod__、__pow__、分别对应+、-、*、/、//、%、**；__and__、__or__、__xor__ 分别对应&amp;、|、^ ；对于以上函数__func__，__ifunc__重载的函数是对应的增量赋值符号，如__iadd__对应+=。 容器方法：__getitem__(self, key)规定了self[key]的行为，__setitem__(self, key, value)规定了self[key] = value的行为，__delitem__(key)规定了del self[key]的行为，__contains__(self, item)规定了in操作符的行为。 容器方法：__getitem__(self, key)规定了self[key]的行为，__setitem__(self, key, value)规定了self[key] = value的行为，__delitem__(key)规定了del self[key]的行为，__contains__(self, item)规定了in操作符的行为。 还有很多魔术方法不再一一列出，以上提到的很多方法都出现在了 collections 的源码中。 3、可迭代对象可迭代对象是指类中定义了__iter__方法和__next__方法的对象，能够通过迭代器遍历该对象，其中__iter__返回自身，__next__返回下一个迭代值。在 Python 的基本数据类型中，list、dict、tuple、str、set、str 等都是可迭代对象，collections 中实现了判断一个对象是否可迭代的方法类Iterable。对于可迭代对象obj，可以通过it = iter(obj)获取其迭代器，并通过next(it)获取其下一个值。用法如下： 12345678910111213141516171819&gt;&gt;&gt; from collections.abc import Iterable&gt;&gt;&gt; class A():... def __init__(self):... self.index = 0... def __iter__(self):... return self... def __next__(self):... if self.index &gt; 9:... raise StopIteration... num = [i * i for i in range(10)][self.index]... self.index += 1... return num...&gt;&gt;&gt; isinstance(A(), Iterable)True&gt;&gt;&gt; for i in A():... print(i, end=' ')...0 1 4 9 16 25 36 49 64 81 4、生成器Python 有两种方法可以定义生成器，一种是利用列表推导式，如(x for x in range(10))，它不会生成一个长度为 10 的列表，而是生成一个 generator ；第二种方法是定义一个含有yield关键字的函数，示例代码如下： 123456789&gt;&gt;&gt; def test(x):... while x &gt;= 0:... yield x... x -= 1...&gt;&gt;&gt; for i in test(10):... print(i, end=' ')...10 9 8 7 6 5 4 3 2 1 0 这里的test方法变成了一个 generator，调用时返回yield对应的值，下次调用时从yield的下一条语句继续执行，直到再次遇到yield或者退出函数，并且 generator 中的内存不会被释放。 yield from语句后面可以跟一个可迭代对象，从而达到循环迭代的效果，如yield from [1, 2, 3]。 二、collections模块简介collections 是 Python 内建的集合模块，提供了一些有关集合的函数和类，其实现位于：Lib\\collections\\__init__.py。 1. OrderedDictOrderedDict 是 dict 的子类，它对于存入的键值对，按照插入顺序进行排序，示例代码如下： 1234567&gt;&gt;&gt; from collections import OrderedDict&gt;&gt;&gt; d = dict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; d{'a': 1, 'c': 3, 'b': 2}&gt;&gt;&gt; od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])&gt;&gt;&gt; odOrderedDict([('a', 1), ('b', 2), ('c', 3)]) 2. namedtuplenamedtuple 是一个函数，它可以根据初始化时给定的名字返回一个动态类型的对象。顾名思义，nametuple 给一个 tuple 加上了名字，并使其能通过属性访问元素。示例代码如下： 12345&gt;&gt;&gt; from collections import namedtuple&gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y'])&gt;&gt;&gt; p = Point(1, 2)&gt;&gt;&gt; print(p.x, p.y)1 2 3. CounterCounter 的本质是一个字典，它可以对 key 值的出现次数进行统计，并将此作为对应的 value 。示例代码如下： 12345678&gt;&gt;&gt; from collections import Counter&gt;&gt;&gt; colors = ['red', 'blue', 'red', 'green', 'blue', 'blue']&gt;&gt;&gt; Counter(colors)Counter({'blue': 3, 'red': 2, 'green': 1})&gt;&gt;&gt; Counter('gallahad')Counter({'a': 3, 'l': 2, 'g': 1, 'h': 1, 'd': 1})&gt;&gt;&gt; Counter('abracadabra').most_common(3)[('a', 5), ('b', 2), ('r', 2)] 4. ChainMapChainMap 可以把多个字典按顺序作为一个单元处理，查找某一个 key 对应的 value 时，按照顺序在这些字典中查找，靠前的字典具有更高的优先级。示例代码如下： 123456&gt;&gt;&gt; from collections import ChainMap&gt;&gt;&gt; a = {'x': 1, 'z': 3 }&gt;&gt;&gt; b = {'y': 2, 'z': 4 }&gt;&gt;&gt; c=ChainMap(a,b)&gt;&gt;&gt; c['z']3 三、模块实现1. OrderedDictOrderedDict 是一个继承于 dict 的类。在构造函数中，定义了自身的根节点__root，以及前驱__root.prev和后继__root.next，并使其指向__root自身，便于下面构造双向循环链表。每个节点都是一个对象_Link()，该对象通过__slots__限制了能够添加的属性列表：['prev', 'next', 'key', '__weakref__']。此外，通过一个字典__map记录key与节点的映射关系。 OrderedDict对象有以下公有方法 clear用于清空链表和__map； popitem用于删除并返回最右端的键值对，即节点__root.prev，当传入的参数last为False时，删除并返回__root.next对应的键值对； move_to_end用于将给定的key值对应的节点，移动到最右端位置__root.prev，当传入的参数last为False时，则移动到最左端__root.next位置； pop允许传入一个key值和一个default，当key值存在时删除相应的键值对并返回对应的值，如果不存在则返回default；setdefault用于为特定的键值设置默认值； copy用于返回一个浅拷贝，通过__class__方法实现。 此外，为了便于遍历key、value和(key, value)，定义了三个辅助类_OrderedDictKeysView、_OrderedDictValuesView、_OrderedDictItemsView，它们分别继承自三个抽象类_collections_abc.KeysView、_collections_abc.ItemsView、_collections_abc.ValuesView。 2. namedtuplenamedtuple 的实现是一个函数，传入字符串参数typename作为构建新类型的名称，如typename='Point'，则namedtuple返回值的类型即为__main__.Point。传入参数field_names是属性名称所组成的列表，或以空格、逗号间隔的字符串，如['x', 'y','z']、'x y z'。 接下来对field_names进行处理，默认情况下其中包含重复、下划线开头、关键字的字符串则会抛出异常，但若传入的参数rename为真，则函数会将不合法的参数重命名为_index（index为字符串在列表中的索引）。 默认情况下，构造对象时必须为每个属性赋予默认值，否则会抛出异常，但可以通过传入defaults参数，为部分或全部属性赋予默认值，在构造对象时只需填充不足的参数数量即可。 为了构造自定义类型的对象result，首先定义好result的内置方法，如_fields、_make、_replace等，再将field_names的每个名称定义成其属性，以上方法和属性均暂存在字典class_namespace中，再利用result = type(typename, (tuple,), class_namespace)构造result，并且继承了 tuple 类。函数返回对象result。 3. CounterCounter 类继承自 dict，主要功能由重新定义的update方法实现。定义 Counter 时传入的初始化参数，将会被update方法处理。若传入参数是抽象类 Mapping 的子类（映射），对于新的 key 值，直接添加键值对；对于已有的 key 值，将 value 值相加；若传入参数是非映射类型，则调用_count_elements方法对参数进行遍历，统计每一个遍历参数的出现次数；若传入参数是关键字参数，如Counter(a=1, b=2)，则调用 dict 自带的update函数进行处理。 Counter 对象有以下公有方法： most_common方法通过对键值对按照 value 值逆向排序，输出 value 值最大的键值对，若传入参数 n，则调用堆排序heapq.nlargest，输出 value 值前 n 大的键值对； elements方法则将键值对转换为一个迭代器，实现方法是利用 itertools 模块中的repeat和startmap方法，将生成的多个可重复迭代器连接在一起； subtract方法允许传入一个可以构造Counter的参数，并用原对象减去参数所代表的Counter，如Counter({'a':1,'b':2,'c':6,'d':9}).subtract({'a':0,'b':1,'c':7,'d':9})的结果为Counter({'a': 1, 'b': 1, 'd': 0, 'c': -1})； copy方法返回一个浅拷贝。 除此之外，Counter类中还重载了很多运算符，如：+、-、+=、&amp;、|等。 4. ChainMapChainMap初始化时，可以传入0个、1个或多个dict，或者是dict组成的列表，构造函数都将这些数据转为dict列表，注意这里是引用式构造，当外部dict改变时会影响ChainMap的数据，反之亦然。 ChainMap对象有以下公有方法： get方法对字典列表进行遍历，查找给定的key值对应的第一个value，若找不到key则返回None或传入的参数default； popitem方法能够删除并返回第一个字典的最后一个键值对；pop方法从第一个字典中删除给定key值对应的键值对，并返回对应的value； clear方法清空第一个字典；new_child方法可以在头部插入一个指定的字典；parents方法返回除了第一个字典之外的字典列表。 四、总结 在利用 Python 编写新的数据结构时，要尽量灵活利用已有的数据结构，以节省工作量、降低数据结构的复杂度。 初步了解了 Python 元编程，以及类的构建原理，即利用元类 Metaclass 来创建 Class，再用 Class 对象创建实例，缺省的 Metaclass 是type。通过type(name, bases, dict)可以动态构造类对象。如： 12345678def instancetest(self): pass@classmethoddef classtest(cls): passtest_property = {&quot;name&quot;: &quot;tom&quot;, &quot;instancetest&quot;: instancetest, &quot;classtest&quot;: classtest}Test = type(&quot;Test&quot;, (), test_property)test = Test() 此外，还可以利用exec方法动态执行 Python 语句字符串，达到类似的效果。 在编写 Python 库时，要充分考虑其易用性，并考虑调用者可能会传入的非法参数，避免预料之外的情况发生，适当地使用try ... except ...语法，给出合适的报错信息。","link":"/2020/09/20/Collections%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"},{"title":"RadASM2配置","text":"RadASM 是一款针对汇编语言的 IDE。 初级配置 Github地址： https://github.com/mrfearless/RadASM2/releases/tag/Latest 下载、安装、打开 选项 – 设置路径 – App($A) 改成自己的 masm32 路径 支持 Irvine32 库：在代码中添加 123includelib YourIrvinePath\\Kernel32.libincludelib YourIrvinePath\\User32.libincludelib YourIrvinePath\\Irvine32.lib 自定义配色：选项 - 颜色及关键字（自带的配色亮瞎眼） 打开安装目录中的 RadASM.ini，找到 [Color] 代码段 亮色，添加： 1{{index}}={{name}},16777215,0,8388608,16777215,15777984,12644544,12632304,16441763,8421504,8388608,14286847,0,14286847,0,14286847,0,8421440,16711680,37781504,4227072,9508725,16809984,25165824,33521664,33489024,29393024,25198592,21004543,16777471,33488896,33133133,33183327,25165888,16711680,25166079,16809984,11942572,25165824,13717099,16777215,16777215,16777215,16777215,16777215,16777215,16777215,16777215,16777215,16777215,16777215,16777215,16777215,16777215,16777215,16777215,16777215,0,0,0 暗色，添加： 1{{index}}={{name}},3549952,9868419,10592659,4339207,6316128,12644544,4605510,8533715,1461195,8388608,3549952,9868419,3549952,9868419,3549952,9868419,3549952,9868419,8421376,33609727,10592659,1461195,2631935,12874092,39301,16744448,8533715,12874092,11184640,1461195,8533715,25310931,12874092,282233638,16711680,16711680,13798182,1461195,13798182,3549952,3549952,3549952,3549952,2039583,2039583,2039583,2039583,2039583,2039583,2039583,2039583,2039583,2039583,2039583,2039583,2039583,2039583,2039583,2039583 自定义字体：选项 - 字体选项 - 代码编辑 - Consolas 编写代码 文件 – 新建工程 编译器 – masm 工程类型 – Console App 工程名称 (hello) 、工程文件夹 下一步 - …… - 完成，在 hello.asm 中编写汇编代码 左下角按钮可以显示行号。 调试代码运行代码：工具栏中提供按钮，进行编译 – 构建 – 运行。快捷键：Ctrl + F5。 使用自带调试工具 编译并构建，在视图 – Debug Window打开调试窗口。 设置行断点（Ctrl + T）。 构建 – Debug – Run（Shift + F7） Step into: F7 Step over: Ctrl + F7 Break: Ctrl + Alt + F7 OllyDbg需要以管理员身份打开 RadASM，以免调用 OllyDbg 时弹出警告。 OllyDbg 官网：http://www.ollydbg.de/ 在 RadASM 中配置：选项 – 设置路径 – Debug($E) 改为自己的 OllyDbg 路径。 开始调试：Ctrl + D 更加强大的功能： 寄存器修改、数值搜索 插件扩展 寻常断点、API断点、内存断点、硬件断点","link":"/2020/10/12/RadASM2%E9%85%8D%E7%BD%AE/"},{"title":"基于Win32的游戏破解与修改","text":"Windows 平台下，游戏中的数据只要存储在本地，而又没有经过精心设计的加密，就很容易用其他进程获取内存，进行修改，并对其进行各种操作。 一、工具与参考资料1、Cheat EngineGithub 地址：https://github.com/cheat-engine/cheat-engine 摘自维基百科的介绍： Cheat Engine，一般简称CE，是一个开放源代码的软件，作者为Eric (“Dark Byte”)，功能包括：内存扫描、十六进制编辑器、调试工具，供Windows和Mac操作系统运行。Cheat Engine最常在电脑游戏中当做游戏外挂，有时会更新以避免被其他软件检测到。透过Cheat Engine，用户可以查找与修改电脑的内存。 2、OllyDBG官方网站：http://www.ollydbg.de/ OllyDBG 是一个32位汇编级分析调试器。本博客中不会用到，但却是比 Cheat Engine 更为强大的反汇编工具，可以用来在必要时进行断点调试。 3、Win32 API文档：https://docs.microsoft.com/zh-cn/windows/win32/ 使用 Win32 提供的 API 可以实现使用编程方式修改内存数据，从而实现现成的修改工具难以完成的自定义任务，比如基于游戏数据自动做出决策。下面举出一个例子：ReadProcessMemory，地址是：ReadProcessMemory function (memoryapi.h)。 Syntax 以 C++ 形式给出了函数声明，可以直接在 C 语言或者 C++ 中调用； 其中可能包含很多新的结构体定义，请自行搜索相关资料。 Parameters 给出了各个参数的描述； Return value 给出了返回值的定义； 有些函数的返回值代表是否成功执行，如果成功返回非零值，否则返回 0 ，此时可以通过函数 GetLastError获得错误代码，各个错误代码的含义可以参考 System Error Codes (0-499)。 在以汇编的形式调用 API 时，返回值存储在 eax 寄存器中。 Remarks 给出了函数说明或者备注； Requirements 给出了使用时需要引用的库。 该函数在 C 中的使用示例： 1int status = ReadProcessMemory(processHandle, (int*)newBaseAddress, &amp;dataBuffer, sizeof(dataBuffer), NULL); 该函数在汇编中的使用示例： 1234invoke ReadProcessMemory, processHandle, addressBuffer, addr dataBuffer, sizeof dataBuffer, NULLcmp eax, 0je myendmov eax, dataBuffer 二、获取程序内存基址1、调用 Win32 API为了简单起见，此处假设进程只有一个模块。 下面是汇编示例代码（略去了参数中各个变量的类型声明，请自行查阅文档）： 1234567891011121314invoke FindWindow, NULL, addr windowNamemov windowHandle, eaxinvoke GetWindowThreadProcessId, windowHandle, addr processIDmov windowThreadID, eaxinvoke OpenProcess, PROCESS_ALL_ACCESS, FALSE, processIDmov processHandle, eaxinvoke EnumProcessModules, processHandle, addr moduleHandleList, sizeof moduleHandleList, addr lpcbNeededmov eax, moduleHandleList[0]mov baseAddress, eaxinvoke CloseHandle, processHandle C 语言版本： 123456789101112int main() { char windowName[] = &quot;2048 Game Professional&quot;; HWND windowHandle = FindWindowA(NULL, windowName); DWORD processID = 0; DWORD windowThreadID = GetWindowThreadProcessId(windowHandle, &amp;processID); HANDLE processHandle = OpenProcess(PROCESS_ALL_ACCESS, 0, processID); HMODULE hMods[1024]; DWORD cbNeeded; BOOL status = EnumProcessModules(processHandle, hMods, sizeof(hMods), &amp;cbNeeded); DWORD baseAddress = hMods[0]; CloseHandle(processHandle);} 以上只是示例各个函数的调用，仅对于简单的程序是可行的，进一步运用参考Enumerating All Modules For a Process。 调用 EnumProcessModules 会将参数中的 hMods 数组进行修改，其中各个数组的值即为各个模块的地址。由于此处仅有一个模块，最后得到的 baseAddress 即为进程基址。 2、使用Cheat EngineCheat Engine 的入门使用就不再赘述，请自行搜索相关资料。 使用 Cheat Engine 打开进程后，选择查看内存： 如果顺利的话，这里的 AllocationBase 就是上一部分所获得的地址。如果不是，检查模块名称是否一致。 三、获取数据基址偏移每次启动程序时，系统都会重新为游戏分配内存，分配内存的起始地址很可能是不同的，但是游戏中数据的地址相对与起始地址却是有规律的。例如： 相对 AllocationBase 的某一个偏移地址 p1 ，指向了某一个地址 p2； p3 是相对于 p2 固定偏移的一个地址，即数据块的基址； p4 是相对于 p3 固定偏移的一个地址，指向的值为游戏中某一个具体数据的值。 例如，要查找游戏中 score 的基址偏移，具体操作如下： 使用 Cheat Engine 搜索出 score 的地址； 右键 - 找出是什么改写了这个地址 - 改变游戏中 score 数值； 记录地址偏移 14004 和 edx 的值 02B55F70，score 的地址就是 edx + 14004； 返回 Cheat Engine 搜索 02B55F70，注意勾选 Hex。如果搜索到了很多个结果，那可能要慢慢尝试或者稍微判断一下了； 右键 - 找出是什么访问了这个地址 - 改变游戏中 score 数值； 记录地址偏移 28024 和 eax 的值 061AD020，返回 Cheat Engine 搜索 061AD020，找到如下结果： 这里说明，程序基址 + C06E1C 总是指向 061AD020。所以，score 的内存地址偏移为 [[.exe + C06E1C] + 28024] + 14004 ，这里的 .exe 即为 AllocationBase，也即在第二部分找到的地址。 四、读取和修改数据使用 Cheat Engine 可以直接修改数据，这里叙述的方法是使用 Win32 API 获取和修改数据。请务必仔细阅读第二部分的代码。 主要使用如下函数： 1234567891011121314151617181920BOOL ReadProcessMemory( HANDLE hProcess, LPCVOID lpBaseAddress, LPVOID lpBuffer, SIZE_T nSize, SIZE_T *lpNumberOfBytesRead);BOOL WriteProcessMemory( HANDLE hProcess, LPVOID lpBaseAddress, LPCVOID lpBuffer, SIZE_T nSize, SIZE_T *lpNumberOfBytesWritten);BOOL PostMessageA( HWND hWnd, UINT Msg, WPARAM wParam, LPARAM lParam); 具体参数定义，文档虽然是英文，但也叙述很清楚。这里主要提醒，ReadProcessMemory 和 WriteProcessMemory 第一个参数为进程句柄，PostMessageA 第一个参数为窗口句柄。 实际上，笔者还尝试了代码注入方法，但过于复杂，需要许多操作系统的知识，就不在此叙述了，有兴趣的读者可以尝试探索。 五、参考 本文使用的示例程序：https://2048-windows-download.en.softonic.com/ Win32 API：https://docs.microsoft.com/zh-cn/windows/win32/ Get base address of process","link":"/2020/11/04/%E5%9F%BA%E4%BA%8EWin32%E7%9A%84%E6%B8%B8%E6%88%8F%E7%A0%B4%E8%A7%A3%E4%B8%8E%E4%BF%AE%E6%94%B9/"},{"title":"Linux 配置踩坑","text":"持续更新 Linux 配置记录。就不用用到的时候再去搜索了。 基础配置更换镜像源常用软件Git1234sudo apt-get install gitgit config --global user.name 'blueice-thu'git config --global user.email 'blueice-thu@outlook.com'ssh-keygen -t rsa -C 'blueice-thu@outlook.com' ssh 密钥存储在 .ssh/id-rsa.pub 中。 查看配置列表： 1git config --global --list 测试： 1ssh -T git@github.com v2ray下载客户端：https://github.com/Qv2ray/Qv2ray 给予权限：sudo chmod +x ./Qv2ray-refs.tags.v1.99.6-linux.AppImage 下载核心：https://github.com/v2ray/v2ray-core 解压，在客户端中设置核心的位置和文件夹","link":"/2021/01/10/Linux-%E9%85%8D%E7%BD%AE%E8%B8%A9%E5%9D%91/"},{"title":"概率基础","text":"最近在学习机器学习中的概率模型，回顾一下需要的概率基础知识。 概率与似然全概率公式：由原因推结果$$P(A)=\\sum_{i}P(A|B_i)P(B_i)$$贝叶斯公式：由结果推原因$$P(B_i|A)=\\frac{P(AB_i)}{P(A)}=\\frac{P(A|B_i)P(B_i)}{\\sum_{j}P(A|B_j)P(B_j)}$$$$p(\\theta|D)=\\frac{p(D|\\theta)p(\\theta)}{\\int p(D|\\theta)p(\\theta)\\text{d}\\theta}$$ 先验分布（Prior）$p(\\theta)$ ：在未看到观测数据的时候参数 $\\theta$ 的不确定性的概率分布 后验分布（Posterior）$p(\\theta|D)$ ：考虑和给出相关证据或数据后所得到的条件概率分布 似然（Likelihood）$p(D|\\theta)$ ：关于统计模型中的参数 $\\theta$ 的函数，表示模型参数中的似然性 对于如下式子：$$L(x_1,…,x_n;\\theta)=f(x_1;\\theta)f(x_2;\\theta)…f(x_n;\\theta)$$ 当 $\\theta$ 固定时， $L$ 是一个概率函数 当 $x$ 固定时，$L$ 是一个似然函数 似然估计：用似然程度最大的 $\\theta^*$ 估计真实的 $\\theta$ 值，即：$$L(X_1,…,X_n;\\theta^*)=\\max_{\\theta}L(X_1,…,X_n;\\theta)$$为了使 $L$ 最大，仅需要 $\\ln L$ 最大，即：$$\\frac{\\partial \\ln L}{\\partial \\theta_i}=\\frac{\\partial \\sum_{i=1}^{n}\\ln f(X_i;\\theta)}{\\partial \\theta_i}=0$$贝叶斯估计：给定一个先验密度 $h(\\theta)$ ，从概率密度中抽取样本 $X_1,X_2,…,X_n$ ，则 $(X_1,X_2,…,X_n)$ 的边缘密度：$$P(X_1,X_2,…,X_n)=\\int h(\\theta)f(X_1,\\theta)f(X_2,\\theta)…f(X_n,\\theta)\\mathrm{d}\\theta$$则 $\\theta$ 的条件密度（后验密度）为：$$h(\\theta|X_1,x_2,…,X_n)=\\frac{h(\\theta)f(X_1,\\theta)f(X_2,\\theta)…f(X_n,\\theta)}{P(X_1,X_2,…,X_n)}$$ 概率分布在贝叶斯统计中，如果后验分布与先验分布属于同类（分布形式相同），则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。 高斯分布 (Gaussian Distribution) ，又名正态分布$$p(y)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}$$伯努利分布 (Bernoulli Distribution) $Ber(p)$ ：$x\\in{0,1}$，$p=P(x=1)$$$P(x|p)=p^x(1-p)^{1-x}$$二项分布 (Binomial Distribution) $B(n, p)$ ：重复 $n$ 次的伯努利分布，随机变量 $x$ 取值 $0,1,2,…,n$$$P(X=i)=C_n^ip^i(1-p)^{n-i}$$泊松分布 (Poisson Distribution) ：给定常数 $\\lambda&gt;0$，随机变量 $x$ 取值 $0,1,2,…$ 。当二项分布的 $n$ 很大而 $p$ 很小时，泊松分布可作为二项分布的近似，其中 $\\lambda=np$$$P(X=i)=e^{-\\lambda}\\lambda^{i}/i!$$类别分布 (Categorical Distribution) $Cat(\\alpha_1,\\alpha_2,…,\\alpha_k)$ ：对于事件的结果 $a_1,a_2,…,a_k$ ， $P(a=a_i)=\\alpha_i$，随机变量 $x$ 取值 $1,2,3,…,k$ ，$\\sum_{j=1}^{k}\\alpha_j=1$$$P(X=j)=\\alpha_j$$One-hot 编码的形式：$P(X_j=1)=\\alpha_j$ 多项分布 (Multinomial Distribution) $Multi(n,\\alpha_1,\\alpha_2,…,\\alpha_k)$ ：重复 $n$ 次的类别分布，对于事件的结果 $a_1,a_2,…,a_k$ ， $P(a=a_i)=\\alpha_i$，$x_i$ 为事件 $a_i$ 的出现次数。$$P(X_1=x_1,X_2=x_2,…,X_k=x_k)=\\frac{n!}{x_1!x_2!…x_k!}\\alpha_1^{x_1}\\alpha_2^{x_2}…\\alpha_k^{x_k}$$Beta 分布 $\\text{Beta}(\\alpha,\\beta)$ ：一组定义在 $(0, 1)$ 区间的连续概率分布，常用于表示概率的概率分布，常用于表示成功或者失败的概率的概率分布，参数 $\\alpha,\\beta&gt;0$$$\\text{Beta}(\\alpha,\\beta)=\\frac{1}{B(\\alpha, \\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}$$在 $\\pi=\\frac{\\alpha-1}{\\alpha+\\beta-2}$ 处取最大值，均值 $E(\\pi)=\\frac{\\alpha}{\\alpha+\\beta}$ 。 狄利克雷分布 (Dirichlet Distribution) ：Beta 分布的多维推广，如 $K$ 维的 Dirichlet 分布：$$Dirichlet(p|\\alpha)=\\frac{\\Gamma\\left(\\sum_{i=1}^{K}\\alpha_i\\right)}{\\Pi_{i=1}^{K}\\Gamma\\left(\\alpha_i\\right)}\\Pi_{i=1}^{K}p_{i}^{\\alpha_i-1}$$ 参考博客 “共轭分布”是什么？ - 知乎 (zhihu.com) 带你理解beta分布_Jie Qiao的专栏-CSDN博客_beta分布 【NLP-04】隐含狄利克雷分布(LDA) - 忆凡人生 - 博客园 (cnblogs.com)","link":"/2021/05/28/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/"},{"title":"2021 微软暑期实习面试","text":"记录一下面试经历。 北京 STCA 软件开发工程师，面试流程是周一一面，表现好就直接周五终面，表现不好还需要周三二面。远程面试，软件是微软自家的 Teams 。 笔试投简历之后会邮件给一个在线笔试的链接。英文题目，四道题目，两个小时。基本是 Leetcode 的 Easy 到 Normal 难度，不考察奇技淫巧，主要考察逻辑能力。最后一道题是动态规划，比较难想，我就随便写了个暴力解法交上去了。全部交上去之后可以看到分数，过程中是看不到分数的，提供的测试样例也就三四个的样子。 一面周一一面自我评价并不好。首先是自我介绍，没怎么准备，介绍得一塌糊涂，印象比较深刻的是说自己是转专业来的，面试官就问是不是转专业之后才接触编程，我连忙说不是。后来一想，我真是脑子进水了提这个…… 问了问我列出来的论文，主要是考察一下我是不是在水吧，我就简单介绍了一下自己了解的部分，具体的细节我是真没准备好，答得一塌糊涂。面试官问数据集如何如何，如何做的零样本学习等等，虽然面的是开发，但还是问了 AI 的相关内容。以前参与论文的时候也是没搞懂这些概念，毕竟基础知识还没掌握，以后还是得花时间读论文补课。 然后就是问八股文了。首先问封装继承和多态，我是真没想到问这些，但我的确知道是什么，于是就凭着自己的理解进行描述，到了多态的部分实在描述不清楚了，只是说参数为指针和引用时会使用其实际类型调用虚函数，就只好说了不好意思，面试官也说我知道你可能理解，但对一个不懂这些概念的人来说肯定是不够的。随后还问了多态的机制，答曰虚函数表。随后问你了解的数据结构有哪些，我就顺着思路列举了一大堆，甚至有单调栈、线段树之类的，现在想来还真是眼高手低、高开低走了…… 接着就是写代码的部分。第一个是简单的二叉树层次遍历，数据结构的内容是真忘了，我用两个数组写了出来，然后面试官说你为什么要用两个数组呢，一个队列就够了，然后我很快改好了。第二个题目是，有一个字符串数组，求这些字符串的最长公共前缀，想了半天并没有好的想法，面试官说你先写个暴力的，我就写了出来。随后面试官说可以用分治算法，然后我写了个分治算法，但是只是大致框架正确，其实还是不对，但是时间快到了，面试官也没说什么。 最后面试官问我有没有什么要问的，我说我明白我基础不太好，您觉得我有哪些地方可以改进。他说你代码写得太少，老是写一些废 code ，要多想。 到了周三仍然没有收到面试通知，我以为我凉了，结果周三晚上收到邮件说进终面了（微软你个浓眉大眼的也加班到十点？），感觉还是因为学历捞了我。终面是 leader 面。 leader 面这次面试就比较顺利了。面试官开门见山，也不用自我介绍。首先问了我简历中的 FTP 项目，问我多个连接怎么实现的，我说收到新的请求时就为这个请求分配一个线程，这个线程负责处理这个连接的各个命令。然后就开始写代码，让我实现智能指针，还算是比较顺利，有一两处小问题，比如 delete 指针后没有把指针设为 nullptr 。然后问我哪里线程不安全，如何解决，我就指出来引用计数的地方不安全，可以加锁。 随后问我一些分布式系统的东西。我就直说我没有学过分布式的相关课程，但会尽力作答。首先就是问怎么知道某个资源存在哪个服务器上，我想到了计算机网络课上学的路由表收敛机制，就套了上去，但是我理解错了面试官的意思，人说用什么数据结构，我顿时明白过来是问哈希表，没想到问得太简单了哈哈哈。然后就是说如果这个资源特别大，哈希表特别大怎么办，我直接就说可以哈希表的值可以存摘要 md5 值。面试官比较满意，说面试挺快的哈，就直接提前结束了。 其实我还特意早起准备了自我介绍、论文介绍等等，就是一面没做好的内容，没想到全没用上…… 两周之后才收到 offer 邮件，随后就是确认、背调、入职手续等等。","link":"/2021/06/06/2021%E5%BE%AE%E8%BD%AF%E6%9A%91%E6%9C%9F%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95/"},{"title":"软件&#x2F;网站清单","text":"常用的软件和网站来备份一下。 PDF目录编辑：PdgCntEditor Sumatra PDF 网络Ping、Traceroute：Best Trace IP 定位：纯真 IP 地址数据库 抓包： Fiddler Everywhere Wireshark 绘图draw.io 免费思维导图：ZhiMap 下载Github 下载工具：DownGit 文件格式文件转换器 论文Overleaf：Overleaf","link":"/2021/06/09/%E8%BD%AF%E4%BB%B6%E6%B8%85%E5%8D%95/"},{"title":"动态规划：背包问题","text":"0-1背包，完全背包，多重背包。 0-1背包背包装载重量 W，物品有 N 个，第 i 个物品的重量为 wt[i]、价值为 val[i]，求价值最多的方案。 dp[i][w]：前 i 个物品、背包容量为 w 时的最大价值。 12345678910111213int dp[N+1][W+1];// 初始化需要视情况而定dp[0][...] = 0;dp[...][0] = 0;for (int i = 1; i &lt;= N; i++) for (int w = 1; w &lt;= W; w++) dp[i][w] = max( dp[i-1][w], // not choose i dp[i-1][w - wt[i-1]] + val[i-1] // choose i )return dp[N][W] 有些问题可以将 dp 简化为一维数组。 变形： 416. 分割等和子集 - 力扣（LeetCode） (leetcode-cn.com) 背包容量为 sum / 2 474. 一和零 - 力扣（LeetCode） (leetcode-cn.com) 494. 目标和 - 力扣（LeetCode） (leetcode-cn.com) a - b = S, a + b = sum =&gt; a = (S + sum) / 2 1049. 最后一块石头的重量 II - 力扣（LeetCode） (leetcode-cn.com) 完全背包最大容量 amount，每个物品的重量为 coins[i] 、数量无限，求将背包装满的方案数量。 dp[i][j]：只使用前 i 个物品、背包容量为 j 时的装满方案数量。 12345678910int dp[coins.size() + 1][amount + 1];dp[...][0] = 1;for (int i = 1; i &lt;= coins.length; i++) for (int j = 1; j &lt;= amount; j++) { dp[i][j] = dp[i-1][j]; if (j &gt;= coins[i-1][j]) { dp[i][j] += dp[i][j - coins[i - 1]]; } }return dp[coins.length][amount] 变形： 322. 零钱兑换 - 力扣（LeetCode） (leetcode-cn.com) 518. 零钱兑换 II - 力扣（LeetCode） (leetcode-cn.com) 多重背包有 N 种物品和容量为 V 的背包，第 i 件物品一共有 n[i] 件可用、费用 / 重量为 c[i] 、价值为 w[i] 。 状态转移方程：f[i][v] = max{f[i-1][v-k*c[i]]+k*w[i] | 0&lt;=k&lt;=n[i]} 1234567891011121314int dp[N+1][V+1];for (int i = 1; i &lt;= N; i++) for (int j = 1; j &lt;= V; j++) { dp[i][j] = dp[i-1][j]; for (int k = 0; k &lt;= n[i]; k++) { if (j - k * c[i - 1] &gt;= 0) { dp[i][j] = max( dp[i][j], dp[i-1][j - k * c[i-1]] + k * w[i-1] ) } } }return dp[N][V] 题目：798 · 背包问题VII - LintCode","link":"/2021/07/03/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%EF%BC%9A%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"汇编","slug":"汇编","link":"/tags/%E6%B1%87%E7%BC%96/"},{"name":"Win32","slug":"Win32","link":"/tags/Win32/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"工作","slug":"工作","link":"/tags/%E5%B7%A5%E4%BD%9C/"},{"name":"备忘","slug":"备忘","link":"/tags/%E5%A4%87%E5%BF%98/"}],"categories":[]}